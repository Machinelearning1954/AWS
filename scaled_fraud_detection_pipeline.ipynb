{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Machine Learning Pipeline for Fraud Detection\n",
    "\n",
    "**Machine Learning Engineering Bootcamp Capstone: Step 8 - Scale Your Prototype with Large-Scale Data**\n",
    "\n",
    "This Jupyter notebook documents the design and implementation of a scalable machine learning pipeline for fraud detection. The pipeline is built using Apache Spark and distributed XGBoost to handle web-scale financial transaction data (conceptually billions of records)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Environment and Execution Notes\n",
    "\n",
    "- **Sandbox Limitation:** This notebook contains PySpark code that requires a Java Development Kit (JDK) and a properly configured Apache Spark environment. The current sandbox environment does **not** have these pre-installed, so the Spark-related code cells **cannot be executed directly within this sandbox.**\n",
    "- **External Execution Required:** To run the Spark pipeline, please refer to the detailed instructions in the `external_run_instructions.md` file. This file explains how to set up a suitable environment (local machine or cluster) with Java, Spark, and the necessary Python dependencies.\n",
    "- **Code Presentation:** The code cells below replicate the logic from the `scaled_fraud_detection_spark.py` script. They are presented here for documentation and review purposes as part of the capstone submission.\n",
    "- **Path Adjustments:** When running externally, you may need to adjust file paths (e.g., for data output and model saving) in the script or notebook to match your local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Goal and Overview\n",
    "\n",
    "The primary goal is to develop a fraud detection system capable of processing and learning from very large volumes of financial transaction data. This involves:\n",
    "1.  Designing a data pipeline that can ingest and preprocess massive datasets efficiently.\n",
    "2.  Training a robust machine learning model (XGBoost) in a distributed manner.\n",
    "3.  Evaluating the model's performance on key metrics relevant to fraud detection (e.g., AUPRC, F1-score, Recall at low FPR).\n",
    "4.  Ensuring the solution is well-documented and demonstrates an understanding of scaling ML algorithms and the associated trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports\n",
    "\n",
    "This section includes the necessary imports for the PySpark and XGBoost pipeline. Ensure these libraries are installed in your external Spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, when, hour, dayofweek, lit, monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Import SparkXGBClassifier (requires xgboost>=1.6.0 with spark module)\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "\n",
    "print(\"Libraries imported successfully (in a compatible environment).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spark Session Creation\n",
    "\n",
    "The following function creates a Spark session. This is the entry point to Spark functionality.\n",
    "**Note:** This cell will fail in the sandbox due to missing Java/Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"Creates and returns a Spark session.\"\"\"\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"ScalableFraudDetection\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")  # Adjust as per your environment's limits\n",
    "        .config(\"spark.executor.memory\", \"2g\") # Adjust as per your environment's limits\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") # For better performance with pandas conversion if used\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark\n",
    "\n",
    "# Example of creating a session (DO NOT RUN IN SANDBOX)\n",
    "# spark = create_spark_session()\n",
    "# print(spark.version) # This would print the Spark version if executed successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Generation (Simulated)\n",
    "\n",
    "For demonstration purposes, we generate a simulated dataset. In a real-world scenario, data would be ingested from sources like HDFS, S3, Kafka, etc. The function below creates a Parquet file with transaction data.\n",
    "\n",
    "**Note:** This cell requires an active Spark session and will fail in the sandbox. The number of rows (`num_rows`) can be adjusted to simulate larger datasets in your external environment. The paths are set for the sandbox; you might need to change `output_path` when running externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulated_data(spark, num_rows=1000000, output_path=\"/home/ubuntu/ml_capstone_scaling/simulated_transactions.parquet\"):\n",
    "    \"\"\"Generates simulated transaction data and saves it as Parquet.\"\"\"\n",
    "    print(f\"Generating {num_rows} simulated transaction records...\")\n",
    "\n",
    "    df = spark.range(num_rows).withColumnRenamed(\"id\", \"_id\")\n",
    "\n",
    "    df = df.withColumn(\"transaction_id\", (rand() * 1000000000).cast(\"int\").cast(\"string\")) \\\n",
    "           .withColumn(\"user_id\", (rand() * 100000).cast(\"int\").cast(\"string\")) \\\n",
    "           .withColumn(\"timestamp\", (lit(time.time()) - rand() * 3600*24*30).cast(\"timestamp\")) \\\n",
    "           .withColumn(\"transaction_amount\", (rand() * 1000 + 5).cast(\"double\")) \\\n",
    "           .withColumn(\"merchant_id\", (rand() * 1000).cast(\"int\").cast(\"string\")) \\\n",
    "           .withColumn(\"location_country\", when(rand() < 0.3, \"US\").when(rand() < 0.6, \"GB\").otherwise(\"CA\")) \\\n",
    "           .withColumn(\"device_type\", when(rand() < 0.5, \"mobile\").when(rand() < 0.8, \"desktop\").otherwise(\"tablet\"))\n",
    "\n",
    "    fraud_percentage = 0.005  # Approx 0.5% fraud for demonstration\n",
    "    df = df.withColumn(\"is_fraud\", when(rand() < fraud_percentage, 1).otherwise(0).cast(\"integer\"))\n",
    "    \n",
    "    df = df.drop(\"_id\")\n",
    "\n",
    "    print(\"Generated data schema:\")\n",
    "    df.printSchema()\n",
    "    print(\"Sample generated data:\")\n",
    "    df.show(5, truncate=False)\n",
    "    print(\"Fraud distribution:\")\n",
    "    df.groupBy(\"is_fraud\").count().show()\n",
    "\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(f\"Simulated data saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Example of data generation (DO NOT RUN IN SANDBOX)\n",
    "# spark = create_spark_session() # Assuming spark session is created\n",
    "# data_path = generate_simulated_data(spark, num_rows=100000, output_path=\"./simulated_transactions.parquet\") # Adjusted path for local run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Ingestion\n",
    "\n",
    "Load the generated (or existing) data from Parquet files into a Spark DataFrame.\n",
    "\n",
    "**Note:** Requires an active Spark session and data generated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming spark session and data_path are available from previous steps in an external environment\n",
    "# print(\"\\n--- Ingesting Data ---\")\n",
    "# raw_df = spark.read.parquet(data_path)\n",
    "# raw_df.printSchema()\n",
    "# raw_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering and Preprocessing Pipeline\n",
    "\n",
    "This section defines the feature engineering and preprocessing steps using Spark MLlib's `Pipeline` API. This includes:\n",
    "- Extracting temporal features (hour, day of week).\n",
    "- Encoding categorical features (StringIndexer + OneHotEncoder).\n",
    "- Scaling numerical features (StandardScaler).\n",
    "- Assembling all features into a single vector.\n",
    "\n",
    "**Note:** Requires an active Spark session and `raw_df` from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming spark and raw_df are available\n",
    "# print(\"\\n--- Feature Engineering & Preprocessing ---\")\n",
    "\n",
    "# # Extract temporal features\n",
    "# df_features = raw_df.withColumn(\"tx_hour\", hour(col(\"timestamp\"))) \\\n",
    "#                     .withColumn(\"tx_dayofweek\", dayofweek(col(\"timestamp\")))\n",
    "# \n",
    "# # Define categorical and numerical columns\n",
    "# categorical_cols = [\"location_country\", \"device_type\"]\n",
    "# numerical_cols = [\"transaction_amount\", \"tx_hour\", \"tx_dayofweek\"]\n",
    "# label_col = \"is_fraud\"\n",
    "# \n",
    "# # Create stages for the pipeline\n",
    "# stages = []\n",
    "# \n",
    "# # StringIndexer and OneHotEncoder for categorical features\n",
    "# for cat_col in categorical_cols:\n",
    "#     string_indexer = StringIndexer(inputCol=cat_col, outputCol=cat_col + \"_index\", handleInvalid=\"keep\")\n",
    "#     one_hot_encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[cat_col + \"_ohe\"])\n",
    "#     stages += [string_indexer, one_hot_encoder]\n",
    "# \n",
    "# # StandardScaler for numerical features\n",
    "# numerical_assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"numerical_features_vec\")\n",
    "# stages.append(numerical_assembler)\n",
    "# scaler = StandardScaler(inputCol=\"numerical_features_vec\", outputCol=\"scaled_numerical_features\")\n",
    "# stages.append(scaler)\n",
    "# \n",
    "# # Assemble all processed features into a single feature vector\n",
    "# feature_cols_for_assembly = [cat_col + \"_ohe\" for cat_col in categorical_cols] + [\"scaled_numerical_features\"]\n",
    "# vector_assembler = VectorAssembler(inputCols=feature_cols_for_assembly, outputCol=\"features\")\n",
    "# stages.append(vector_assembler)\n",
    "# \n",
    "# # Create the preprocessing pipeline\n",
    "# preprocessing_pipeline = Pipeline(stages=stages)\n",
    "# \n",
    "# # Fit the preprocessing pipeline and transform the data\n",
    "# print(\"Fitting preprocessing pipeline... (Requires Spark execution)\")\n",
    "# # preprocessing_model = preprocessing_pipeline.fit(df_features)\n",
    "# # processed_df = preprocessing_model.transform(df_features)\n",
    "# \n",
    "# # print(\"Schema after preprocessing:\")\n",
    "# # processed_df.printSchema()\n",
    "# # print(\"Sample of processed data (selected columns):\")\n",
    "# # processed_df.select(\"features\", label_col).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Splitting\n",
    "\n",
    "Split the processed data into training and test sets.\n",
    "\n",
    "**Note:** Requires `processed_df` from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming processed_df is available\n",
    "# print(\"\\n--- Splitting Data ---\")\n",
    "# # train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "# # print(f\"Training data count: {train_df.count()}\")\n",
    "# # print(f\"Test data count: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training (Distributed XGBoost)\n",
    "\n",
    "Train an XGBoost classifier in a distributed manner using `SparkXGBClassifier`. This handles class imbalance using `scale_pos_weight`.\n",
    "\n",
    "**Note:** Requires `train_df` from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_df, label_col are available\n",
    "# print(\"\\n--- Training XGBoost Model ---\")\n",
    "# \n",
    "# # Calculate scale_pos_weight for imbalanced data\n",
    "# # num_positives = train_df.filter(col(label_col) == 1).count()\n",
    "# # num_negatives = train_df.filter(col(label_col) == 0).count()\n",
    "# # scale_pos_weight_val = float(num_negatives) / num_positives if num_positives > 0 else 1.0\n",
    "# # print(f\"Calculated scale_pos_weight: {scale_pos_weight_val}\")\n",
    "# \n",
    "# xgb_classifier = SparkXGBClassifier(\n",
    "#     featuresCol=\"features\",\n",
    "#     labelCol=label_col,\n",
    "#     eval_metric=\"aucpr\",  # Area Under Precision-Recall Curve\n",
    "#     # scale_pos_weight=scale_pos_weight_val, # Pass the calculated value\n",
    "#     use_gpu=False, # Set to True if GPUs are available and configured in Spark\n",
    "#     seed=42\n",
    "# )\n",
    "# \n",
    "# print(\"Starting XGBoost training... (Requires Spark execution)\")\n",
    "# start_time = time.time()\n",
    "# # xgb_model = xgb_classifier.fit(train_df) # Train on preprocessed training data\n",
    "# end_time = time.time()\n",
    "# # print(f\"XGBoost training completed in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "\n",
    "Evaluate the trained model on the test set using AUPRC, AUROC, Precision, Recall, and F1-score.\n",
    "\n",
    "**Note:** Requires `xgb_model` and `test_df` from previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming xgb_model, test_df, label_col are available\n",
    "# print(\"\\n--- Evaluating Model ---\")\n",
    "# # predictions_df = xgb_model.transform(test_df)\n",
    "# \n",
    "# # print(\"Sample predictions:\")\n",
    "# # predictions_df.select(\"features\", label_col, \"rawPrediction\", \"probability\", \"prediction\").show(5, truncate=False)\n",
    "# \n",
    "# # evaluator_auprc = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"probability\", metricName=\"areaUnderPR\")\n",
    "# # auprc = evaluator_auprc.evaluate(predictions_df.select(label_col, col(\"probability\")))\n",
    "# # print(f\"Area Under Precision-Recall Curve (AUPRC) on Test Data: {auprc:.4f}\")\n",
    "# \n",
    "# # evaluator_auroc = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"probability\", metricName=\"areaUnderROC\")\n",
    "# # auroc = evaluator_auroc.evaluate(predictions_df.select(label_col, col(\"probability\")))\n",
    "# # print(f\"Area Under ROC Curve (AUROC) on Test Data: {auroc:.4f}\")\n",
    "# \n",
    "# # # Calculate Precision, Recall, F1 for the positive class (fraud=1)\n",
    "# # tp = predictions_df.filter((col(label_col) == 1) & (col(\"prediction\") == 1.0)).count()\n",
    "# # fp = predictions_df.filter((col(label_col) == 0) & (col(\"prediction\") == 1.0)).count()\n",
    "# # fn = predictions_df.filter((col(label_col) == 1) & (col(\"prediction\") == 0.0)).count()\n",
    "# # \n",
    "# # precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "# # recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "# # f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "# # \n",
    "# # print(f\"Precision for fraud class: {precision:.4f}\")\n",
    "# # print(f\"Recall for fraud class: {recall:.4f}\")\n",
    "# # print(f\"F1-score for fraud class: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Saving (Conceptual)\n",
    "\n",
    "Save the trained XGBoost model and the preprocessing pipeline model. This allows for later use in batch inference or deployment.\n",
    "\n",
    "**Note:** Requires `xgb_model` and `preprocessing_model` from previous steps. Paths might need adjustment for external execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming xgb_model and preprocessing_model are available\n",
    "# print(\"\\n--- Saving Model (Conceptual) ---\")\n",
    "# model_output_path = \"/home/ubuntu/ml_capstone_scaling/spark_xgb_fraud_model\" # Adjust path if needed\n",
    "# preprocessing_model_path = \"/home/ubuntu/ml_capstone_scaling/spark_preprocessing_model\" # Adjust path if needed\n",
    "# \n",
    "# # xgb_model.write().overwrite().save(model_output_path)\n",
    "# # print(f\"Trained XGBoost model saved to {model_output_path}\")\n",
    "# # \n",
    "# # preprocessing_model.write().overwrite().save(preprocessing_model_path)\n",
    "# # print(f\"Preprocessing pipeline model saved to {preprocessing_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Stopping Spark Session\n",
    "\n",
    "It's important to stop the Spark session to release resources.\n",
    "\n",
    "**Note:** Requires an active `spark` session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming spark session is active\n",
    "# print(\"\\n--- Stopping Spark Session ---\")\n",
    "# # spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Discussion on Scalability and Trade-offs\n",
    "\n",
    "This pipeline is designed for scalability by leveraging Apache Spark's distributed processing capabilities:\n",
    "\n",
    "- **Data Ingestion & Processing:** Spark DataFrames can handle terabyte-scale datasets distributed across a cluster. Operations are performed in a parallel and fault-tolerant manner.\n",
    "- **Feature Engineering:** Spark SQL and DataFrame API allow for complex feature transformations on large datasets without memory limitations of single-node tools like pandas.\n",
    "- **Distributed Training:** `SparkXGBClassifier` trains the XGBoost model using data parallelism across Spark executors. This significantly reduces training time for large datasets compared to single-node XGBoost.\n",
    "- **Choice of Tools/Libraries:** 
",
    "    - **Apache Spark:** Chosen for its mature ecosystem, scalability, and fault tolerance for big data processing.
",
    "    - **XGBoost (Spark integration):** Chosen for its state-of-the-art performance in classification tasks, especially with tabular data, and its ability to be trained in a distributed fashion.
",
    "    - **Parquet:** Used as the storage format for its efficiency with columnar data, compression, and predicate pushdown capabilities, which are beneficial for Spark performance.\n",
    "- **Choice of ML Technique:** XGBoost is a gradient boosting algorithm known for its high accuracy and efficiency. Its distributed version allows it to scale to large datasets. The use of `scale_pos_weight` helps address class imbalance common in fraud detection.\n",
    "\n",
    "### Trade-offs Considered:\n",
    "\n",
    "- **Complexity:** Distributed systems are inherently more complex to set up, manage, and debug than single-node solutions. However, for web-scale data, this complexity is a necessary trade-off for performance and capability.\n",
    "- **Resource Requirements:** Training models on billions of data points requires significant computational resources (CPU, memory, network bandwidth). Cloud platforms offer elasticity but come with associated costs.\n",
    "- **Development Time:** Setting up and optimizing Spark jobs can take more time initially compared to using familiar single-node libraries. However, the long-term benefits for large-scale data processing are substantial.\n",
    "- **Hyperparameter Tuning:** While Spark MLlib provides `CrossValidator`, tuning distributed models can still be computationally intensive. The number of parameters and folds must be chosen judiciously.\n",
    "\n",
    "### Handling Web-Scale Data (Billions of Data Points):\n",
    "\n",
    "The presented pipeline, when executed in a properly configured Spark cluster (e.g., on AWS EMR, GCP Dataproc, Azure HDInsight, or an on-premise Hadoop cluster with sufficient resources), is capable of handling billions of data points:\n",
    "\n",
    "1.  **Data Storage:** Data would reside in a distributed file system (HDFS, S3, GCS, ADLS) partitioned for efficient access.\n",
    "2.  **Data Processing:** Spark executors would process partitions of the data in parallel. The number of executors and their resources (cores, memory) would be configured based on the dataset size and cluster capacity.\n",
    "3.  **XGBoost Training:** The `SparkXGBClassifier` would distribute the training process. The `num_workers` parameter (or inferred by Spark) would determine the degree of parallelism for training.\n",
    "4.  **Memory Management:** Spark's ability to spill data to disk and its optimized execution engine (Tungsten) help manage memory even for datasets larger than aggregate RAM, though performance is best when data can fit in memory.\n",
    "\n",
    "This design meets the criteria of scaling an ML prototype to handle large volumes of data and demonstrates well-thought-out decisions regarding tools, libraries, and ML techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

